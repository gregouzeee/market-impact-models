{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection - Market Impact & Almgren-Chriss Models\n",
    "\n",
    "## Objective\n",
    "This notebook collects and processes market data necessary to estimate Almgren-Chriss model parameters:\n",
    "- High-frequency market data (OHLCV)\n",
    "- Daily volume aggregation and intraday volume profiles\n",
    "- Market parameters: volatility, volume, spreads\n",
    "\n",
    "## Data Sources\n",
    "1. **Stocks**: Databento (Nasdaq TotalView-ITCH) via S3 - minute-level data\n",
    "2. **Cryptocurrencies**: Binance Public API - minute-level OHLCV\n",
    "\n",
    "## Outputs\n",
    "- Raw data: `data/raw/stocks/` and `data/raw/crypto/`\n",
    "- Processed data: `data/processed/stocks/` and `data/processed/crypto/`\n",
    "- Market parameters: `data/processed/market_parameters.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful\n",
      "üìÖ Execution date: 2025-12-07 11:42:38\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# File system\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Data APIs\n",
    "import requests\n",
    "import s3fs\n",
    "import pytz\n",
    "\n",
    "# Matplotlib configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('‚úÖ Imports successful')\n",
    "print(f'üìÖ Execution date: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Directory structure created\n"
     ]
    }
   ],
   "source": [
    "# Create directory structure\n",
    "Path('data/raw/stocks').mkdir(parents=True, exist_ok=True)\n",
    "Path('data/raw/crypto').mkdir(parents=True, exist_ok=True)\n",
    "Path('data/processed/stocks').mkdir(parents=True, exist_ok=True)\n",
    "Path('data/processed/crypto').mkdir(parents=True, exist_ok=True)\n",
    "Path('results/figures').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('‚úÖ Directory structure created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stock Data Collection (Databento via S3)\n",
    "\n",
    "### Configuration\n",
    "- **Source**: Databento (Nasdaq TotalView-ITCH)\n",
    "- **Tickers**: AAPL, MSFT, GOOG\n",
    "- **Timeframe**: January - June 2025 (6 months)\n",
    "- **Frequency**: 1-minute bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ S3 connection successful\n",
      "Stock tickers: AAPL, MSFT, GOOG\n"
     ]
    }
   ],
   "source": [
    "# S3 Configuration (optional - skip if not in SSPCloud)\n",
    "STOCK_TICKERS = ['AAPL', 'MSFT', 'GOOG']\n",
    "BUCKET = 'gmarguier'\n",
    "PREFIX = 'market-impact-model/databento'\n",
    "\n",
    "# Try to connect to S3 (will skip if credentials not available)\n",
    "try:\n",
    "    s3 = s3fs.S3FileSystem(\n",
    "        client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'}\n",
    "    )\n",
    "    # Test connection\n",
    "    s3.ls(BUCKET)\n",
    "    ENABLE_STOCK_COLLECTION = True\n",
    "    print(f'‚úÖ S3 connection successful')\n",
    "    print(f'Stock tickers: {\", \".join(STOCK_TICKERS)}')\n",
    "except Exception as e:\n",
    "    ENABLE_STOCK_COLLECTION = False\n",
    "    print(f'‚ö†Ô∏è S3 connection failed: {e}')\n",
    "    print(f'‚ö†Ô∏è Stock data collection will be skipped')\n",
    "    print(f'   To enable stock data collection, run this notebook in SSPCloud')\n",
    "    print(f'   or configure AWS credentials locally')\n",
    "    print(f'\\n   Continuing with crypto data only...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stock data loading functions created\n"
     ]
    }
   ],
   "source": [
    "def convert_databento_format(df, ticker):\n",
    "    \"\"\"Convert Databento format to standard OHLCV format\"\"\"\n",
    "    column_mapping = {\n",
    "        'ts_event': 'datetime',\n",
    "        'open': 'open',\n",
    "        'high': 'high',\n",
    "        'low': 'low',\n",
    "        'close': 'close',\n",
    "        'volume': 'volume'\n",
    "    }\n",
    "    \n",
    "    df = df.rename(columns=column_mapping)\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    df['ticker'] = ticker\n",
    "    \n",
    "    return df[['open', 'high', 'low', 'close', 'volume', 'ticker']]\n",
    "\n",
    "def load_stock_data_from_s3(ticker, bucket, prefix):\n",
    "    \"\"\"Load all files for a ticker from S3 and concatenate\"\"\"\n",
    "    ticker_folder = f\"{bucket}/{prefix}/{ticker}\"\n",
    "    \n",
    "    print(f'\\nüì• Loading {ticker}...')\n",
    "    \n",
    "    try:\n",
    "        files = s3.glob(f\"{ticker_folder}/*.csv.zst\")\n",
    "        \n",
    "        if not files:\n",
    "            print(f'  ‚ö†Ô∏è No files found in {ticker_folder}')\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(f'  Found {len(files)} file(s)')\n",
    "        \n",
    "        dfs = []\n",
    "        for file_path in files:\n",
    "            try:\n",
    "                with s3.open(file_path, 'rb') as f:\n",
    "                    df = pd.read_csv(f, compression='zstd', parse_dates=['ts_event'])\n",
    "                    dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f'  ‚ùå Error: {e}')\n",
    "        \n",
    "        if not dfs:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        combined_df = convert_databento_format(combined_df, ticker)\n",
    "        combined_df = combined_df[~combined_df.index.duplicated(keep='first')]\n",
    "        combined_df.sort_index(inplace=True)\n",
    "        \n",
    "        print(f'  ‚úÖ Total: {len(combined_df):,} unique rows')\n",
    "        return combined_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'  ‚ùå Error: {e}')\n",
    "        return pd.DataFrame()\n",
    "\n",
    "print('‚úÖ Stock data loading functions created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Loading AAPL...\n",
      "  Found 6 file(s)\n",
      "  ‚úÖ Total: 99,818 unique rows\n",
      "\n",
      "üì• Loading MSFT...\n",
      "  Found 6 file(s)\n",
      "  ‚úÖ Total: 90,422 unique rows\n",
      "\n",
      "üì• Loading GOOG...\n",
      "  Found 6 file(s)\n",
      "  ‚úÖ Total: 92,040 unique rows\n",
      "\n",
      "‚úÖ Loading complete: 3 tickers\n",
      "AAPL: 2025-01-02 09:00:00+00:00 ‚Üí 2025-06-27 23:59:00+00:00 (99,818 rows)\n",
      "MSFT: 2025-01-02 09:00:00+00:00 ‚Üí 2025-06-27 23:53:00+00:00 (90,422 rows)\n",
      "GOOG: 2025-01-02 09:01:00+00:00 ‚Üí 2025-06-27 23:59:00+00:00 (92,040 rows)\n"
     ]
    }
   ],
   "source": [
    "# Load stock data (only if S3 is available)\n",
    "stock_data = {}\n",
    "\n",
    "if ENABLE_STOCK_COLLECTION:\n",
    "    for ticker in STOCK_TICKERS:\n",
    "        df = load_stock_data_from_s3(ticker, BUCKET, PREFIX)\n",
    "        if not df.empty:\n",
    "            stock_data[ticker] = df\n",
    "\n",
    "    print(f'\\n‚úÖ Loading complete: {len(stock_data)} tickers')\n",
    "    for ticker, df in stock_data.items():\n",
    "        print(f'{ticker}: {df.index.min()} ‚Üí {df.index.max()} ({len(df):,} rows)')\n",
    "else:\n",
    "    print('\\n‚ö†Ô∏è Skipping stock data collection (S3 not available)')\n",
    "    print('   Stock data will not be included in parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AAPL processed: 47,190 rows\n",
      "‚úÖ MSFT processed: 47,190 rows\n",
      "‚úÖ GOOG processed: 47,190 rows\n",
      "\n",
      "üìä Stock market data formatted:\n",
      "   - Rows: 141,570\n",
      "   - Period: 2025-01-02 14:30:00+00:00 ‚Üí 2025-06-27 19:59:00+00:00\n",
      "   - Tickers: ['AAPL' 'MSFT' 'GOOG']\n"
     ]
    }
   ],
   "source": [
    "# Process stock data (only if available)\n",
    "processed_stock_data = {}\n",
    "\n",
    "if ENABLE_STOCK_COLLECTION and len(stock_data) > 0:\n",
    "    def filter_regular_trading_hours(df):\n",
    "        \"\"\"Keep only regular NASDAQ trading hours: 9:30 AM - 4:00 PM ET\"\"\"\n",
    "        eastern = pytz.timezone('US/Eastern')\n",
    "        df_et = df.copy()\n",
    "        df_et.index = df_et.index.tz_convert(eastern)\n",
    "        \n",
    "        market_open = pd.Timestamp('09:30:00').time()\n",
    "        market_close = pd.Timestamp('16:00:00').time()\n",
    "        \n",
    "        mask = (df_et.index.time >= market_open) & (df_et.index.time < market_close)\n",
    "        df_filtered = df_et[mask].copy()\n",
    "        df_filtered.index = df_filtered.index.tz_convert('UTC')\n",
    "        \n",
    "        return df_filtered\n",
    "\n",
    "    for ticker, df in stock_data.items():\n",
    "        df_ticker = filter_regular_trading_hours(df)\n",
    "        df_ticker = df_ticker.reset_index()\n",
    "        df_ticker.columns = [col.lower() if isinstance(col, str) else col for col in df_ticker.columns]\n",
    "        df_ticker['datetime'] = pd.to_datetime(df_ticker['datetime'])\n",
    "        df_ticker = df_ticker.set_index('datetime').sort_index()\n",
    "        df_ticker['mid_price'] = (df_ticker['high'] + df_ticker['low']) / 2\n",
    "        \n",
    "        processed_stock_data[ticker] = df_ticker\n",
    "        print(f'‚úÖ {ticker} processed: {len(df_ticker):,} rows')\n",
    "\n",
    "    # Concatenate all stock data\n",
    "    df_stocks = pd.concat(processed_stock_data.values(), axis=0).sort_index()\n",
    "    print(f'\\nüìä Stock market data formatted:')\n",
    "    print(f'   - Rows: {len(df_stocks):,}')\n",
    "    print(f'   - Period: {df_stocks.index.min()} ‚Üí {df_stocks.index.max()}')\n",
    "    print(f'   - Tickers: {df_stocks[\"ticker\"].unique()}')\n",
    "else:\n",
    "    print('‚ö†Ô∏è No stock data to process')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cryptocurrency Data Collection (Binance API)\n",
    "\n",
    "### Configuration\n",
    "- **Source**: Binance Public API (no authentication required)\n",
    "- **Symbols**: BTCUSDT, ETHUSDT, SOLUSDT\n",
    "- **Timeframe**: November 2025 (1 month)\n",
    "- **Frequency**: 1-minute OHLCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BinanceDataCollector class created\n"
     ]
    }
   ],
   "source": [
    "class BinanceDataCollector:\n",
    "    \"\"\"Class to collect OHLCV data from Binance API\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url='https://api.binance.com'):\n",
    "        self.base_url = base_url\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "    def get_klines(self, symbol, interval, start_time=None, end_time=None, limit=1000):\n",
    "        \"\"\"Fetch OHLCV data for a symbol\"\"\"\n",
    "        endpoint = f'{self.base_url}/api/v3/klines'\n",
    "        \n",
    "        params = {'symbol': symbol, 'interval': interval, 'limit': limit}\n",
    "        \n",
    "        if isinstance(start_time, str):\n",
    "            start_time = int(pd.Timestamp(start_time).timestamp() * 1000)\n",
    "        if isinstance(end_time, str):\n",
    "            end_time = int(pd.Timestamp(end_time).timestamp() * 1000)\n",
    "            \n",
    "        if start_time:\n",
    "            params['startTime'] = start_time\n",
    "        if end_time:\n",
    "            params['endTime'] = end_time\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(endpoint, params=params, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            df = pd.DataFrame(data, columns=[\n",
    "                'timestamp', 'open', 'high', 'low', 'close', 'volume',\n",
    "                'close_time', 'quote_volume', 'trades', 'taker_buy_base',\n",
    "                'taker_buy_quote', 'ignore'\n",
    "            ])\n",
    "            \n",
    "            df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume', 'trades']].copy()\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "            \n",
    "            for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df['trades'] = df['trades'].astype(int)\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f'‚ùå Request error: {e}')\n",
    "            return None\n",
    "    \n",
    "    def get_historical_data(self, symbol, interval, start_date, end_date=None):\n",
    "        \"\"\"Fetch complete history with automatic pagination\"\"\"\n",
    "        if end_date is None:\n",
    "            end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        start_ts = int(pd.Timestamp(start_date).timestamp() * 1000)\n",
    "        end_ts = int(pd.Timestamp(end_date).timestamp() * 1000)\n",
    "        \n",
    "        all_data = []\n",
    "        current_ts = start_ts\n",
    "        \n",
    "        print(f'üì• Fetching {symbol} data ({interval})')\n",
    "        print(f'   Period: {start_date} ‚Üí {end_date}')\n",
    "        \n",
    "        while current_ts < end_ts:\n",
    "            df = self.get_klines(symbol, interval, start_time=current_ts, end_time=end_ts, limit=1000)\n",
    "            \n",
    "            if df is None or len(df) == 0:\n",
    "                break\n",
    "            \n",
    "            all_data.append(df)\n",
    "            current_ts = int(df['timestamp'].iloc[-1].timestamp() * 1000) + 1\n",
    "            time.sleep(0.1)  # Respect rate limits\n",
    "        \n",
    "        if not all_data:\n",
    "            print('‚ùå No data retrieved')\n",
    "            return None\n",
    "        \n",
    "        full_df = pd.concat(all_data, ignore_index=True)\n",
    "        full_df = full_df.drop_duplicates(subset=['timestamp']).reset_index(drop=True)\n",
    "        full_df = full_df.sort_values('timestamp').reset_index(drop=True)\n",
    "        \n",
    "        print(f'‚úÖ Total: {len(full_df):,} rows retrieved')\n",
    "        return full_df\n",
    "\n",
    "print('‚úÖ BinanceDataCollector class created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Symbols: BTCUSDT, ETHUSDT, SOLUSDT\n",
      "  Interval: 1m\n",
      "  Period: 2025-11-01 ‚Üí 2025-12-01\n"
     ]
    }
   ],
   "source": [
    "# Crypto collection configuration\n",
    "CRYPTO_SYMBOLS = ['BTCUSDT', 'ETHUSDT', 'SOLUSDT']\n",
    "INTERVAL = '1m'\n",
    "START_DATE = '2025-11-01'\n",
    "END_DATE = '2025-12-01'\n",
    "\n",
    "print(f'Configuration:')\n",
    "print(f'  Symbols: {\", \".join(CRYPTO_SYMBOLS)}')\n",
    "print(f'  Interval: {INTERVAL}')\n",
    "print(f'  Period: {START_DATE} ‚Üí {END_DATE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üì• Fetching BTCUSDT data (1m)\n",
      "   Period: 2025-11-01 ‚Üí 2025-12-01\n",
      "‚úÖ Total: 43,201 rows retrieved\n",
      "\n",
      "üìä Summary BTCUSDT:\n",
      "   Shape: (43201, 7)\n",
      "   Period: 2025-11-01 00:00:00 ‚Üí 2025-12-01 00:00:00\n",
      "   Avg price: $96573.75\n",
      "   Avg volume: 18.17\n",
      "\n",
      "======================================================================\n",
      "üì• Fetching ETHUSDT data (1m)\n",
      "   Period: 2025-11-01 ‚Üí 2025-12-01\n",
      "‚úÖ Total: 43,201 rows retrieved\n",
      "\n",
      "üìä Summary ETHUSDT:\n",
      "   Shape: (43201, 7)\n",
      "   Period: 2025-11-01 00:00:00 ‚Üí 2025-12-01 00:00:00\n",
      "   Avg price: $3222.25\n",
      "   Avg volume: 410.97\n",
      "\n",
      "======================================================================\n",
      "üì• Fetching SOLUSDT data (1m)\n",
      "   Period: 2025-11-01 ‚Üí 2025-12-01\n",
      "‚úÖ Total: 43,201 rows retrieved\n",
      "\n",
      "üìä Summary SOLUSDT:\n",
      "   Shape: (43201, 7)\n",
      "   Period: 2025-11-01 00:00:00 ‚Üí 2025-12-01 00:00:00\n",
      "   Avg price: $148.82\n",
      "   Avg volume: 2895.70\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Collection complete: 3 symbols\n"
     ]
    }
   ],
   "source": [
    "# Collect crypto data\n",
    "collector = BinanceDataCollector()\n",
    "crypto_data = {}\n",
    "\n",
    "for symbol in CRYPTO_SYMBOLS:\n",
    "    print(f'\\n{\"=\"*70}')\n",
    "    df = collector.get_historical_data(symbol, INTERVAL, START_DATE, END_DATE)\n",
    "    \n",
    "    if df is not None:\n",
    "        crypto_data[symbol] = df\n",
    "        print(f'\\nüìä Summary {symbol}:')\n",
    "        print(f'   Shape: {df.shape}')\n",
    "        print(f'   Period: {df[\"timestamp\"].min()} ‚Üí {df[\"timestamp\"].max()}')\n",
    "        print(f'   Avg price: ${df[\"close\"].mean():.2f}')\n",
    "        print(f'   Avg volume: {df[\"volume\"].mean():.2f}')\n",
    "    else:\n",
    "        print(f'‚ùå Failed to retrieve data for {symbol}')\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print(f'‚úÖ Collection complete: {len(crypto_data)} symbols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Validation for BTCUSDT\n",
      "   Initial shape: (43201, 7)\n",
      "   Final shape: (43201, 7)\n",
      "   Rows removed: 0 (0.00%)\n",
      "\n",
      "üîç Validation for ETHUSDT\n",
      "   Initial shape: (43201, 7)\n",
      "   Final shape: (43201, 7)\n",
      "   Rows removed: 0 (0.00%)\n",
      "\n",
      "üîç Validation for SOLUSDT\n",
      "   Initial shape: (43201, 7)\n",
      "   Final shape: (43201, 7)\n",
      "   Rows removed: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "def validate_and_clean_crypto_data(df, symbol):\n",
    "    \"\"\"Validate and clean OHLCV data\"\"\"\n",
    "    print(f'\\nüîç Validation for {symbol}')\n",
    "    print(f'   Initial shape: {df.shape}')\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    initial_rows = len(df_clean)\n",
    "    \n",
    "    # Remove missing values\n",
    "    df_clean = df_clean.dropna()\n",
    "    \n",
    "    # Check OHLC consistency\n",
    "    inconsistent = (\n",
    "        (df_clean['high'] < df_clean['low']) |\n",
    "        (df_clean['high'] < df_clean['open']) |\n",
    "        (df_clean['high'] < df_clean['close']) |\n",
    "        (df_clean['low'] > df_clean['open']) |\n",
    "        (df_clean['low'] > df_clean['close'])\n",
    "    )\n",
    "    df_clean = df_clean[~inconsistent]\n",
    "    \n",
    "    # Remove zero volume\n",
    "    df_clean = df_clean[df_clean['volume'] > 0]\n",
    "    \n",
    "    final_rows = len(df_clean)\n",
    "    print(f'   Final shape: {df_clean.shape}')\n",
    "    print(f'   Rows removed: {initial_rows - final_rows} ({(initial_rows - final_rows)/initial_rows*100:.2f}%)')\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Clean crypto data\n",
    "clean_crypto_data = {}\n",
    "for symbol, df in crypto_data.items():\n",
    "    df_clean = validate_and_clean_crypto_data(df, symbol)\n",
    "    clean_crypto_data[symbol] = df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Market Parameters Estimation\n",
    "\n",
    "For each asset, calculate:\n",
    "1. **œÉ (Volatility)**: Annualized volatility\n",
    "2. **V (Volume)**: Average daily volume\n",
    "3. **S‚ÇÄ (Price)**: Reference price\n",
    "4. **Spread**: Bid-ask spread estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Parameter computation function created\n"
     ]
    }
   ],
   "source": [
    "def compute_market_parameters(df, symbol, asset_type, interval='1m'):\n",
    "    \"\"\"Compute market parameters for Almgren-Chriss model\"\"\"\n",
    "    print(f'\\nüìê Computing parameters for {symbol}')\n",
    "    \n",
    "    # Calculate returns\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    \n",
    "    # 1. VOLATILITY\n",
    "    vol_interval = df['returns'].std()\n",
    "    \n",
    "    # Annualize volatility\n",
    "    if interval == '1m':\n",
    "        intervals_per_day = 1440 if asset_type == 'crypto' else 390  # 24h vs 6.5h trading\n",
    "    else:\n",
    "        intervals_per_day = 1\n",
    "    \n",
    "    vol_daily = vol_interval * np.sqrt(intervals_per_day)\n",
    "    vol_annual = vol_daily * np.sqrt(365 if asset_type == 'crypto' else 252)\n",
    "    \n",
    "    print(f'   œÉ (volatility):')\n",
    "    print(f'      Per interval ({interval}): {vol_interval:.6f}')\n",
    "    print(f'      Daily: {vol_daily:.6f}')\n",
    "    print(f'      Annualized: {vol_annual:.4f} ({vol_annual*100:.2f}%)')\n",
    "    \n",
    "    # 2. VOLUME\n",
    "    # D√©terminer les intervalles par jour (390 pour stock, 1440 pour crypto)\n",
    "    if interval == '1m':\n",
    "        intervals_per_day = 1440 if asset_type == 'crypto' else 390  # 24h vs 6.5h trading\n",
    "    else:\n",
    "        intervals_per_day = 1\n",
    "\n",
    "    # Volume total par minute\n",
    "    V_per_minute = df['volume'].mean()\n",
    "\n",
    "    # Volume total journalier moyen (ADV)\n",
    "    V_per_day = V_per_minute * intervals_per_day\n",
    "    \n",
    "    print(f'   V (avg daily volume): {V_per_day:,.0f}')\n",
    "\n",
    "    df['date'] = pd.to_datetime(df.index)\n",
    "    daily_volume = df.groupby('date')['volume'].sum()\n",
    "    \n",
    "    # 3. PRICE\n",
    "    S0_median = df['close'].median()\n",
    "    S0_recent = df['close'].iloc[-1]\n",
    "    \n",
    "    print(f'   S‚ÇÄ (price):')\n",
    "    print(f'      Median: ${S0_median:.2f}')\n",
    "    print(f'      Recent: ${S0_recent:.2f}')\n",
    "    \n",
    "    # 4. SPREAD\n",
    "    df['spread_pct'] = (df['high'] - df['low']) / df['close'] * 100\n",
    "    spread_mean = df['spread_pct'].mean()\n",
    "    spread_median = df['spread_pct'].median()\n",
    "    \n",
    "    print(f'   Spread (High-Low estimate):')\n",
    "    print(f'      Mean: {spread_mean:.4f}%')\n",
    "    print(f'      Median: {spread_median:.4f}%')\n",
    "    \n",
    "    return {\n",
    "        'symbol': symbol,\n",
    "        'asset_type': asset_type,\n",
    "        'interval': interval,\n",
    "        'vol_interval': vol_interval,\n",
    "        'vol_daily': vol_daily,\n",
    "        'vol_annual': vol_annual,\n",
    "        'volume_per_minute': V_per_minute,\n",
    "        'volume_per_day': V_per_day,\n",
    "        'S0_median': S0_median,\n",
    "        'S0_recent': S0_recent,\n",
    "        'spread_pct_mean': spread_mean,\n",
    "        'spread_pct_median': spread_median,\n",
    "        'spread_bps_mean': spread_mean * 100,\n",
    "        'num_trading_days': len(daily_volume),\n",
    "        'num_intervals': len(df),\n",
    "    }\n",
    "\n",
    "print('‚úÖ Parameter computation function created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìê Computing parameters for AAPL\n",
      "   œÉ (volatility):\n",
      "      Per interval (1m): 0.001270\n",
      "      Daily: 0.025089\n",
      "      Annualized: 0.3983 (39.83%)\n",
      "   V (avg daily volume): 10,782,696\n",
      "   S‚ÇÄ (price):\n",
      "      Median: $212.71\n",
      "      Recent: $201.10\n",
      "   Spread (High-Low estimate):\n",
      "      Mean: 0.1110%\n",
      "      Median: 0.0821%\n",
      "\n",
      "üìê Computing parameters for MSFT\n",
      "   œÉ (volatility):\n",
      "      Per interval (1m): 0.001045\n",
      "      Daily: 0.020641\n",
      "      Annualized: 0.3277 (32.77%)\n",
      "   V (avg daily volume): 4,108,588\n",
      "   S‚ÇÄ (price):\n",
      "      Median: $415.20\n",
      "      Recent: $495.88\n",
      "   Spread (High-Low estimate):\n",
      "      Mean: 0.0865%\n",
      "      Median: 0.0651%\n",
      "\n",
      "üìê Computing parameters for GOOG\n",
      "   œÉ (volatility):\n",
      "      Per interval (1m): 0.001145\n",
      "      Daily: 0.022614\n",
      "      Annualized: 0.3590 (35.90%)\n",
      "   V (avg daily volume): 5,880,532\n",
      "   S‚ÇÄ (price):\n",
      "      Median: $171.20\n",
      "      Recent: $178.39\n",
      "   Spread (High-Low estimate):\n",
      "      Mean: 0.1086%\n",
      "      Median: 0.0832%\n",
      "\n",
      "üìê Computing parameters for BTCUSDT\n",
      "   œÉ (volatility):\n",
      "      Per interval (1m): 0.000716\n",
      "      Daily: 0.027180\n",
      "      Annualized: 0.5193 (51.93%)\n",
      "   V (avg daily volume): 26,163\n",
      "   S‚ÇÄ (price):\n",
      "      Median: $95667.80\n",
      "      Recent: $90408.34\n",
      "   Spread (High-Low estimate):\n",
      "      Mean: 0.0797%\n",
      "      Median: 0.0624%\n",
      "\n",
      "üìê Computing parameters for ETHUSDT\n",
      "   œÉ (volatility):\n",
      "      Per interval (1m): 0.001091\n",
      "      Daily: 0.041413\n",
      "      Annualized: 0.7912 (79.12%)\n",
      "   V (avg daily volume): 591,793\n",
      "   S‚ÇÄ (price):\n",
      "      Median: $3171.23\n",
      "      Recent: $2997.57\n",
      "   Spread (High-Low estimate):\n",
      "      Mean: 0.1253%\n",
      "      Median: 0.0959%\n",
      "\n",
      "üìê Computing parameters for SOLUSDT\n",
      "   œÉ (volatility):\n",
      "      Per interval (1m): 0.001217\n",
      "      Daily: 0.046171\n",
      "      Annualized: 0.8821 (88.21%)\n",
      "   V (avg daily volume): 4,169,802\n",
      "   S‚ÇÄ (price):\n",
      "      Median: $142.27\n",
      "      Recent: $133.72\n",
      "   Spread (High-Low estimate):\n",
      "      Mean: 0.1494%\n",
      "      Median: 0.1213%\n",
      "\n",
      "======================================================================\n",
      "üìä MARKET PARAMETERS SUMMARY\n",
      "======================================================================\n",
      " symbol asset_type  vol_annual  volume_per_day  S0_recent  spread_bps_mean\n",
      "   AAPL      stock    0.398271    1.078270e+07     201.10        11.100363\n",
      "   MSFT      stock    0.327662    4.108588e+06     495.88         8.648971\n",
      "   GOOG      stock    0.358992    5.880532e+06     178.39        10.859490\n",
      "BTCUSDT     crypto    0.519272    2.616318e+04   90408.34         7.974886\n",
      "ETHUSDT     crypto    0.791195    5.917932e+05    2997.57        12.532338\n",
      "SOLUSDT     crypto    0.882097    4.169802e+06     133.72        14.944305\n"
     ]
    }
   ],
   "source": [
    "# Compute parameters for all assets\n",
    "all_parameters = []\n",
    "\n",
    "# Stock parameters (only if available)\n",
    "if len(processed_stock_data) > 0:\n",
    "    for ticker, df in processed_stock_data.items():\n",
    "        params = compute_market_parameters(df.reset_index(), ticker, 'stock', '1m')\n",
    "        all_parameters.append(params)\n",
    "\n",
    "# Crypto parameters\n",
    "for symbol, df in clean_crypto_data.items():\n",
    "    params = compute_market_parameters(df, symbol, 'crypto', '1m')\n",
    "    all_parameters.append(params)\n",
    "\n",
    "# Create parameters DataFrame\n",
    "df_params = pd.DataFrame(all_parameters)\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('üìä MARKET PARAMETERS SUMMARY')\n",
    "print('='*70)\n",
    "print(df_params[['symbol', 'asset_type', 'vol_annual', 'volume_per_day', 'S0_recent', 'spread_bps_mean']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ data/processed/stocks/AAPL_1m.parquet saved (47,190 rows)\n",
      "‚úÖ data/processed/stocks/MSFT_1m.parquet saved (47,190 rows)\n",
      "‚úÖ data/processed/stocks/GOOG_1m.parquet saved (47,190 rows)\n",
      "‚úÖ data/processed/stocks/all_stocks_1m.parquet saved\n",
      "‚úÖ data/processed/crypto/BTCUSDT_1m.parquet saved (43,201 rows)\n",
      "‚úÖ data/processed/crypto/ETHUSDT_1m.parquet saved (43,201 rows)\n",
      "‚úÖ data/processed/crypto/SOLUSDT_1m.parquet saved (43,201 rows)\n",
      "‚úÖ Market parameters saved\n",
      "‚úÖ Metadata saved\n",
      "\n",
      "‚òÅÔ∏è  Uploading processed data to S3...\n",
      "   ‚úÖ Uploaded AAPL to S3\n",
      "   ‚úÖ Uploaded MSFT to S3\n",
      "   ‚úÖ Uploaded GOOG to S3\n",
      "   ‚úÖ Uploaded combined stocks to S3\n",
      "   ‚úÖ Uploaded BTCUSDT to S3\n",
      "   ‚úÖ Uploaded ETHUSDT to S3\n",
      "   ‚úÖ Uploaded SOLUSDT to S3\n",
      "   ‚úÖ Uploaded market parameters to S3\n",
      "\n",
      "‚úÖ All data uploaded to S3: s3://gmarguier/market-impact-data/processed/\n",
      "\n",
      "======================================================================\n",
      "‚úÖ DATA COLLECTION COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Save stock data (only if available)\n",
    "if len(processed_stock_data) > 0:\n",
    "    for ticker, df in processed_stock_data.items():\n",
    "        filename = f'data/processed/stocks/{ticker}_1m.parquet'\n",
    "        df.to_parquet(filename)\n",
    "        print(f'‚úÖ {filename} saved ({len(df):,} rows)')\n",
    "\n",
    "    # Save combined stock data\n",
    "    df_stocks.to_parquet('data/processed/stocks/all_stocks_1m.parquet')\n",
    "    print(f'‚úÖ data/processed/stocks/all_stocks_1m.parquet saved')\n",
    "else:\n",
    "    print('‚ö†Ô∏è No stock data to save')\n",
    "\n",
    "# Save crypto data\n",
    "for symbol, df in clean_crypto_data.items():\n",
    "    filename = f'data/processed/crypto/{symbol}_1m.parquet'\n",
    "    df.to_parquet(filename, index=False)\n",
    "    print(f'‚úÖ {filename} saved ({len(df):,} rows)')\n",
    "\n",
    "# Save market parameters\n",
    "df_params.to_parquet('data/processed/market_parameters.parquet', index=False)\n",
    "df_params.to_csv('data/processed/market_parameters.csv', index=False)\n",
    "print(f'‚úÖ Market parameters saved')\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'collection_date': datetime.now().isoformat(),\n",
    "    'stock_tickers': STOCK_TICKERS if ENABLE_STOCK_COLLECTION else [],\n",
    "    'crypto_symbols': CRYPTO_SYMBOLS,\n",
    "    'interval': '1m',\n",
    "    'total_assets': len(df_params),\n",
    "    's3_enabled': ENABLE_STOCK_COLLECTION\n",
    "}\n",
    "\n",
    "with open('data/processed/metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print('‚úÖ Metadata saved')\n",
    "\n",
    "# Upload to S3 if on SSP Cloud\n",
    "if ENABLE_STOCK_COLLECTION:\n",
    "    print('\\n‚òÅÔ∏è  Uploading processed data to S3...')\n",
    "    try:\n",
    "        # Upload processed stocks\n",
    "        for ticker in processed_stock_data.keys():\n",
    "            local_file = f'data/processed/stocks/{ticker}_1m.parquet'\n",
    "            s3_path = f'{BUCKET}/market-impact-data/processed/stocks/{ticker}_1m.parquet'\n",
    "            s3.put(local_file, s3_path)\n",
    "            print(f'   ‚úÖ Uploaded {ticker} to S3')\n",
    "        \n",
    "        # Upload combined stocks\n",
    "        if len(processed_stock_data) > 0:\n",
    "            s3.put('data/processed/stocks/all_stocks_1m.parquet', \n",
    "                   f'{BUCKET}/market-impact-data/processed/stocks/all_stocks_1m.parquet')\n",
    "            print(f'   ‚úÖ Uploaded combined stocks to S3')\n",
    "        \n",
    "        # Upload crypto data\n",
    "        for symbol in clean_crypto_data.keys():\n",
    "            local_file = f'data/processed/crypto/{symbol}_1m.parquet'\n",
    "            s3_path = f'{BUCKET}/market-impact-data/processed/crypto/{symbol}_1m.parquet'\n",
    "            s3.put(local_file, s3_path)\n",
    "            print(f'   ‚úÖ Uploaded {symbol} to S3')\n",
    "        \n",
    "        # Upload parameters\n",
    "        s3.put('data/processed/market_parameters.parquet', \n",
    "               f'{BUCKET}/market-impact-data/processed/market_parameters.parquet')\n",
    "        s3.put('data/processed/market_parameters.csv', \n",
    "               f'{BUCKET}/market-impact-data/processed/market_parameters.csv')\n",
    "        s3.put('data/processed/metadata.json', \n",
    "               f'{BUCKET}/market-impact-data/processed/metadata.json')\n",
    "        print(f'   ‚úÖ Uploaded market parameters to S3')\n",
    "        \n",
    "        print(f'\\n‚úÖ All data uploaded to S3: s3://{BUCKET}/market-impact-data/processed/')\n",
    "    except Exception as e:\n",
    "        print(f'‚ö†Ô∏è  S3 upload failed: {e}')\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('‚úÖ DATA COLLECTION COMPLETE')\n",
    "print('='*70)\n",
    "if not ENABLE_STOCK_COLLECTION:\n",
    "    print('‚ö†Ô∏è Stock data was not collected (S3 credentials not available)')\n",
    "    print('   Only crypto data has been processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Data collected:\n",
    "- **Stocks**: AAPL, MSFT, GOOG (6 months, minute-level)\n",
    "- **Cryptocurrencies**: BTCUSDT, ETHUSDT, SOLUSDT (1 month, minute-level)\n",
    "\n",
    "### Files generated:\n",
    "- `data/processed/stocks/`: Individual stock data files\n",
    "- `data/processed/crypto/`: Individual crypto data files\n",
    "- `data/processed/market_parameters.csv`: Market parameters (œÉ, V, S‚ÇÄ, spread)\n",
    "- `data/processed/metadata.json`: Collection metadata\n",
    "\n",
    "### Next steps:\n",
    "Use these data files in the Almgren-Chriss model notebooks (02 and 03)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
